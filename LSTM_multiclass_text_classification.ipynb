{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LSTM_multiclass_text_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nolll77/LSTM_multiclass_text_classification/blob/master/LSTM_multiclass_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBWolfWZbAt7",
        "colab_type": "text"
      },
      "source": [
        "# LSTM in Pytorch\n",
        "\n",
        "![Predicting item ratings based on customer reviews](https://drive.google.com/uc?id=1fi3BRAmlN7ljSnZLz2TuZmz9qZstZPxi)\n",
        "\n",
        "\n",
        "Human language is filled with ambiguity, many-a-times the same phrase can have multiple interpretations based on the context and can even appear confusing to humans. Such challenges make natural language processing an interesting but hard problem to solve. However, we’ve seen a lot of advancement in NLP in the past couple of years and it’s quite fascinating to explore the various techniques being used. This article aims to cover one such technique in deep learning using Pytorch: Long Short Term Memory (LSTM) models.\n",
        "\n",
        "If you’re new to NLP or need an in-depth read on preprocessing and word embeddings, you can check out the following [article](https://towardsdatascience.com/getting-started-with-natural-language-processing-nlp-2c482420cc05)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CphM93sb_HA",
        "colab_type": "text"
      },
      "source": [
        "## Gentle Intro to RNNs and LSTMs :\n",
        "\n",
        "What sets language models apart from conventional neural networks is their dependency on context. Conventional feed-forward networks assume inputs to be independent of one another. For NLP, we need a mechanism to be able to use sequential information from previous inputs to determine the current output. Recurrent Neural Networks (RNNs) tackle this problem by having loops, allowing information to persist through the network.\n",
        "\n",
        "![An unrolled Recurrent Neural Network](https://drive.google.com/uc?id=1vFo5VFVHYGM4PTrKNVH3--iXJ-bNOD7u)\n",
        "\n",
        "However, conventional RNNs have the issue of exploding and vanishing gradients and are not good at processing long sequences because they suffer from short term memory.\n",
        "\n",
        "Long Short Term Memory networks (LSTM) are a special kind of RNN, which are capable of learning long-term dependencies. They do so by maintaining an internal memory state called the “cell state” and have regulators called “gates” to control the flow of information inside each LSTM unit. \n",
        "\n",
        "[Here](https://towardsdatascience.com/getting-started-with-natural-language-processing-nlp-2c482420cc05)’s an excellent source explaining the specifics of LSTMs.\n",
        "\n",
        "\n",
        "![Structure of an LSTM cell. (source : Varsamopoulos, Savvas & Bertels, Koen & Almudever, Carmen. (2018). Designing neural network based decoders for surface codes.)\n",
        "](https://drive.google.com/uc?id=1bXZ4F4OporY_NeanJ1m0sZMVMSBf8WJL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtQQgmoCbAt9",
        "colab_type": "code",
        "colab": {},
        "outputId": "ed704de9-9de9-4673-926b-c7e12518751e"
      },
      "source": [
        "#library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import jovian\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEy_MbT0bAuG",
        "colab_type": "text"
      },
      "source": [
        "## Basic LSTM in Pytorch with random numbers\n",
        "\n",
        "Before we jump into the main problem, let’s take a look at the basic structure of an LSTM in Pytorch, using a random input. This is a useful step to perform before getting into complex inputs because it helps us learn how to debug the model better, check if dimensions add up and ensure that our model is working as expected.\n",
        "\n",
        "Even though we’re going to be dealing with text, since our model can only work with numbers, we convert the input into a sequence of numbers where each number represents a particular word (more on this in the next section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKbssHHNbAuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input\n",
        "x = torch.tensor([[1,2, 12,34, 56,78, 90,80],\n",
        "                 [12,45, 99,67, 6,23, 77,82],\n",
        "                 [3,24, 6,99, 12,56, 21,22]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOBa-b3mbAuM",
        "colab_type": "text"
      },
      "source": [
        "#### using two different models\n",
        "\n",
        "We first pass the input (3×8) through an [embedding](https://en.wikipedia.org/wiki/Word_embedding) layer, because word embeddings are better at capturing context and are spatially more efficient than one-hot vector representations.\n",
        "\n",
        "In Pytorch, we can use the nn.Embedding module to create this layer, which takes the vocabulary size and desired word-vector length as input. You can optionally provide a padding index, to indicate the index of the padding element in the embedding matrix.\n",
        "\n",
        "In the following example, our vocabulary consists of 100 words, so our input to the embedding layer can only be from 0–100, and it returns us a 100×7 embedding matrix, with the 0th index representing our padding element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpj39mVxbAuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = nn.Embedding(100, 7, padding_idx=0)\n",
        "model2 = nn.LSTM(input_size=7, hidden_size=3, num_layers=1, batch_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJVix9ZvbAuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out1 = model1(x)\n",
        "out2 = model2(out1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I3uAZbwbAuW",
        "colab_type": "code",
        "colab": {},
        "outputId": "8dfc24e6-fa43-47b6-a747-7f69650bb5bb"
      },
      "source": [
        "print(out1.shape)\n",
        "print(out1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 8, 7])\n",
            "tensor([[[-0.6816, -0.3904, -0.0229,  1.2287, -0.4489, -0.1448,  0.1876],\n",
            "         [-0.1136, -0.2161, -0.6440,  1.7220, -1.0028, -0.4189,  1.4022],\n",
            "         [ 0.1740, -0.2212,  1.5228, -1.1506, -1.1710,  0.0406, -0.3912],\n",
            "         [ 1.9387,  0.3619, -0.4921, -0.6929, -0.6253,  1.1100,  0.8697],\n",
            "         [-1.1030,  0.5688, -0.2015, -1.0526,  2.9643,  1.2638,  1.9368],\n",
            "         [-0.3143, -0.8116, -0.1972,  0.9615,  0.8048, -0.2469,  1.0350],\n",
            "         [ 0.2626, -0.4890, -1.0185,  0.4583,  0.6501, -0.1358,  0.1586],\n",
            "         [-0.4704,  1.3602,  0.6796,  0.4018, -0.2171,  2.0806, -1.0199]],\n",
            "\n",
            "        [[ 0.1740, -0.2212,  1.5228, -1.1506, -1.1710,  0.0406, -0.3912],\n",
            "         [-0.9500,  0.7904,  0.0888, -1.0316, -0.7365, -0.8333,  0.6342],\n",
            "         [-1.0811,  0.2237, -0.4557,  0.4708,  0.8445, -1.0519,  0.0446],\n",
            "         [ 1.3037, -1.0439, -0.8036,  0.5445,  1.7022,  0.7845,  0.0318],\n",
            "         [ 0.8269, -0.6542, -0.3596,  1.8055, -0.8318,  0.6261,  0.2298],\n",
            "         [-0.2241,  0.2127,  0.1145,  0.1325,  0.3162,  0.4276,  0.5688],\n",
            "         [ 1.5142,  1.5675,  0.4787,  0.1893,  1.3999,  0.3825,  0.2888],\n",
            "         [ 0.2900, -1.8883,  0.1017,  0.7807,  2.0393, -0.2231,  0.7619]],\n",
            "\n",
            "        [[ 0.5877,  0.1631, -1.4762,  1.0529, -0.0842,  1.5817,  1.0293],\n",
            "         [ 1.1253,  1.9566, -0.8565,  0.0533, -1.3300,  0.4598, -0.6800],\n",
            "         [ 0.8269, -0.6542, -0.3596,  1.8055, -0.8318,  0.6261,  0.2298],\n",
            "         [-1.0811,  0.2237, -0.4557,  0.4708,  0.8445, -1.0519,  0.0446],\n",
            "         [ 0.1740, -0.2212,  1.5228, -1.1506, -1.1710,  0.0406, -0.3912],\n",
            "         [-1.1030,  0.5688, -0.2015, -1.0526,  2.9643,  1.2638,  1.9368],\n",
            "         [ 1.2083,  0.1338,  1.0553, -1.6460, -1.6378, -0.5144, -1.0399],\n",
            "         [-0.6197, -1.5587,  1.3634,  1.0663,  1.7736, -0.6517, -2.2486]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvKtDsCzmAsJ",
        "colab_type": "text"
      },
      "source": [
        "##### We pass the embedding layer’s output into an LSTM layer (created using nn.LSTM), which takes as input the word-vector length, length of the hidden state vector and number of layers. Additionally, if the first element in our input’s shape has the batch size, we can specify batch_first = True\n",
        "\n",
        "The LSTM layer outputs three things:\n",
        "\n",
        "The consolidated output — of all hidden states in the sequence\n",
        "\n",
        "*   The consolidated output — of all hidden states in the sequence\n",
        "*   Hidden state of the last LSTM unit — the final output\n",
        "*   Cell state\n",
        "\n",
        "Hidden state of the last LSTM unit — the final output\n",
        "Cell state\n",
        "\n",
        "\n",
        "We can verify that after passing through all layers, our output has the expected dimensions:\n",
        "\n",
        "3×8 -> embedding -> 3x8x7 -> LSTM (with hidden size=3)-> 3×3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1KXmI48bAub",
        "colab_type": "code",
        "colab": {},
        "outputId": "9a6c115a-d438-40de-d29c-1262d2f300c7"
      },
      "source": [
        "out, (ht, ct) = model2(out1)\n",
        "print(ht)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2788,  0.1724,  0.0143],\n",
            "         [-0.2855,  0.1841,  0.2382],\n",
            "         [-0.5367,  0.0766,  0.0969]]], grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0lAnVsYbAui",
        "colab_type": "text"
      },
      "source": [
        "#### using nn.sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G1Z8IMzbAuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3 = nn.Sequential(nn.Embedding(100, 7, padding_idx=0),\n",
        "                        nn.LSTM(input_size=7, hidden_size=3, num_layers=1, batch_first=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlPUA67QbAup",
        "colab_type": "code",
        "colab": {},
        "outputId": "474ddb6f-6675-4c7f-f2ec-99feca4d8774"
      },
      "source": [
        "out, (ht, ct) = model3(x)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.2137,  0.0261, -0.1449],\n",
            "         [-0.3795, -0.0441, -0.0928],\n",
            "         [-0.3562, -0.0813, -0.0430],\n",
            "         [-0.3664, -0.0105, -0.2085],\n",
            "         [-0.5796, -0.0787, -0.3419],\n",
            "         [-0.4169, -0.1716, -0.3466],\n",
            "         [-0.4096, -0.0531, -0.5017],\n",
            "         [-0.6825, -0.0244, -0.3172]],\n",
            "\n",
            "        [[-0.1501, -0.0516, -0.0249],\n",
            "         [ 0.0922, -0.0493, -0.0280],\n",
            "         [ 0.1433, -0.1797, -0.0193],\n",
            "         [ 0.3654, -0.1216, -0.0491],\n",
            "         [ 0.4690, -0.1234, -0.0121],\n",
            "         [ 0.3195, -0.0079, -0.1690],\n",
            "         [ 0.2842, -0.0074, -0.2765],\n",
            "         [ 0.0661, -0.0064, -0.2403]],\n",
            "\n",
            "        [[-0.2451, -0.1169, -0.2873],\n",
            "         [-0.3352, -0.0299, -0.0743],\n",
            "         [-0.0965, -0.0810, -0.1175],\n",
            "         [ 0.0841, -0.2123, -0.0219],\n",
            "         [-0.0312, -0.2138, -0.0325],\n",
            "         [-0.2179, -0.1398, -0.2310],\n",
            "         [-0.1087, -0.0455, -0.0574],\n",
            "         [-0.1863, -0.1855, -0.1135]]], grad_fn=<TransposeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YZG146NbAut",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass Text Classification - Predicting ratings from review comments\n",
        "\n",
        "Let’s now look at an application of LSTMs.\n",
        "\n",
        "Problem Statement: Given an item’s review comment, predict the rating ( takes integer values from 1 to 5, 1 being worst and 5 being best)\n",
        "\n",
        "We are going to predict item ratings based on customer reviews bsed on this dataset from Kaggle :\n",
        "\n",
        "[Here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) or [here](https://towardsdatascience.com/getting-started-with-natural-language-processing-nlp-2c482420cc05)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW4jvLDhbAuu",
        "colab_type": "code",
        "colab": {},
        "outputId": "8802e587-583e-405a-b553-8561fcc6dd01"
      },
      "source": [
        "#loading the data\n",
        "reviews = pd.read_csv(\"reviews.csv\")\n",
        "print(reviews.shape)\n",
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23486, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Clothing ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Title</th>\n",
              "      <th>Review Text</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Recommended IND</th>\n",
              "      <th>Positive Feedback Count</th>\n",
              "      <th>Division Name</th>\n",
              "      <th>Department Name</th>\n",
              "      <th>Class Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>767</td>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Initmates</td>\n",
              "      <td>Intimate</td>\n",
              "      <td>Intimates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1080</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1077</td>\n",
              "      <td>60</td>\n",
              "      <td>Some major design flaws</td>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1049</td>\n",
              "      <td>50</td>\n",
              "      <td>My favorite buy!</td>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General Petite</td>\n",
              "      <td>Bottoms</td>\n",
              "      <td>Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>847</td>\n",
              "      <td>47</td>\n",
              "      <td>Flattering shirt</td>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Blouses</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
              "0           0          767   33                      NaN   \n",
              "1           1         1080   34                      NaN   \n",
              "2           2         1077   60  Some major design flaws   \n",
              "3           3         1049   50         My favorite buy!   \n",
              "4           4          847   47         Flattering shirt   \n",
              "\n",
              "                                         Review Text  Rating  Recommended IND  \\\n",
              "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
              "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
              "2  I had such high hopes for this dress and reall...       3                0   \n",
              "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
              "4  This shirt is very flattering to all due to th...       5                1   \n",
              "\n",
              "   Positive Feedback Count   Division Name Department Name Class Name  \n",
              "0                        0       Initmates        Intimate  Intimates  \n",
              "1                        4         General         Dresses    Dresses  \n",
              "2                        0         General         Dresses    Dresses  \n",
              "3                        0  General Petite         Bottoms      Pants  \n",
              "4                        6         General            Tops    Blouses  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OSPJbOibAuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['Title'] = reviews['Title'].fillna('')\n",
        "reviews['Review Text'] = reviews['Review Text'].fillna('')\n",
        "reviews['review'] = reviews['Title'] + ' ' + reviews['Review Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCnmB2fNbAu2",
        "colab_type": "code",
        "colab": {},
        "outputId": "4348839d-f190-455f-f080-615cca9e2b68"
      },
      "source": [
        "#keeping only relevant columns and calculating sentence lengths\n",
        "reviews = reviews[['review', 'Rating']]\n",
        "reviews.columns = ['review', 'rating']\n",
        "reviews['review_length'] = reviews['review'].apply(lambda x: len(x.split()))\n",
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Absolutely wonderful - silky and sexy and com...</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happen...</td>\n",
              "      <td>5</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Some major design flaws I had such high hopes ...</td>\n",
              "      <td>3</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My favorite buy! I love, love, love this jumps...</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Flattering shirt This shirt is very flattering...</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  rating  review_length\n",
              "0   Absolutely wonderful - silky and sexy and com...       4              8\n",
              "1   Love this dress!  it's sooo pretty.  i happen...       5             62\n",
              "2  Some major design flaws I had such high hopes ...       3            102\n",
              "3  My favorite buy! I love, love, love this jumps...       5             25\n",
              "4  Flattering shirt This shirt is very flattering...       5             38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IL1Td2ebAu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#changing ratings to 0-numbering\n",
        "zero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
        "reviews['rating'] = reviews['rating'].apply(lambda x: zero_numbering[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMS5zjKXbAu_",
        "colab_type": "code",
        "colab": {},
        "outputId": "104f23db-bc92-4c83-ea0f-f1e18ad37a40"
      },
      "source": [
        "#mean sentence length\n",
        "np.mean(reviews['review_length'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60.832921740611425"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IBXBdrbns20",
        "colab_type": "text"
      },
      "source": [
        "#### Metric\n",
        "We usually take accuracy as our metric for most classification problems, however, ratings are ordered. If the actual value is 5 but the model predicts a 4, it is not considered as bad as predicting a 1. Hence, instead of going with accuracy, we choose RMSE — root mean squared error as our North Star metric. Also, rating prediction is a pretty hard problem, even for humans, so a prediction of being off by just 1 point or lesser is considered pretty good.\n",
        "\n",
        "Preprocessing\n",
        "As mentioned earlier, we need to convert our text into a numerical form that can be fed to our model as input. I’ve used spacy for tokenization after removing punctuation, special characters, and lower casing the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tg8KerVbAvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenization\n",
        "tok = spacy.load('en')\n",
        "def tokenize (text):\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
        "    nopunct = regex.sub(\" \", text.lower())\n",
        "    return [token.text for token in tok.tokenizer(nopunct)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBL1zic6n43k",
        "colab_type": "text"
      },
      "source": [
        "#### We count the number of occurrences of each token in our corpus and get rid of the ones that don’t occur too frequently :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8_FUXF_bAvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#count number of occurences of each word\n",
        "counts = Counter()\n",
        "for index, row in reviews.iterrows():\n",
        "    counts.update(tokenize(row['review']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5HKRiUoUDF",
        "colab_type": "text"
      },
      "source": [
        "#### We lost about 6000 words! This is expected because our corpus is quite small, less than 25k reviews, the chance of having repeated words is quite small.\n",
        "\n",
        "We then create a vocabulary to index mapping and encode our review text using this mapping. I’ve chosen the maximum length of any review to be 70 words because the average length of reviews was around 60."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Pgx_QLbAvO",
        "colab_type": "code",
        "colab": {},
        "outputId": "f4951d97-58df-4843-efd3-ca7995ef08d0"
      },
      "source": [
        "#deleting infrequent words\n",
        "print(\"num_words before:\",len(counts.keys()))\n",
        "for word in list(counts):\n",
        "    if counts[word] < 2:\n",
        "        del counts[word]\n",
        "print(\"num_words after:\",len(counts.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_words before: 14138\n",
            "num_words after: 8263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4U-5IrTbAvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating vocabulary\n",
        "vocab2index = {\"\":0, \"UNK\":1}\n",
        "words = [\"\", \"UNK\"]\n",
        "for word in counts:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZWG9qjUbAvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(text, vocab2index, N=70):\n",
        "    tokenized = tokenize(text)\n",
        "    encoded = np.zeros(N, dtype=int)\n",
        "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
        "    length = min(N, len(enc1))\n",
        "    encoded[:length] = enc1[:length]\n",
        "    return encoded, length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-J-gX7mbAva",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ad3d7ad-6260-4bcd-a16d-36b3e35870bf"
      },
      "source": [
        "reviews['encoded'] = reviews['review'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_length</th>\n",
              "      <th>encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Absolutely wonderful - silky and sexy and com...</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>[[2, 3, 4, 5, 6, 7, 8, 7, 9, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happen...</td>\n",
              "      <td>4</td>\n",
              "      <td>62</td>\n",
              "      <td>[[2, 10, 11, 12, 5, 13, 14, 15, 16, 5, 17, 18,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Some major design flaws I had such high hopes ...</td>\n",
              "      <td>2</td>\n",
              "      <td>102</td>\n",
              "      <td>[[54, 55, 56, 57, 17, 58, 59, 60, 61, 62, 11, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My favorite buy! I love, love, love this jumps...</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>[[68, 109, 110, 2, 17, 10, 2, 10, 2, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Flattering shirt This shirt is very flattering...</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>[[122, 123, 11, 123, 52, 92, 122, 19, 124, 125...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  rating  review_length  \\\n",
              "0   Absolutely wonderful - silky and sexy and com...       3              8   \n",
              "1   Love this dress!  it's sooo pretty.  i happen...       4             62   \n",
              "2  Some major design flaws I had such high hopes ...       2            102   \n",
              "3  My favorite buy! I love, love, love this jumps...       4             25   \n",
              "4  Flattering shirt This shirt is very flattering...       4             38   \n",
              "\n",
              "                                             encoded  \n",
              "0  [[2, 3, 4, 5, 6, 7, 8, 7, 9, 0, 0, 0, 0, 0, 0,...  \n",
              "1  [[2, 10, 11, 12, 5, 13, 14, 15, 16, 5, 17, 18,...  \n",
              "2  [[54, 55, 56, 57, 17, 58, 59, 60, 61, 62, 11, ...  \n",
              "3  [[68, 109, 110, 2, 17, 10, 2, 10, 2, 10, 11, 1...  \n",
              "4  [[122, 123, 11, 123, 52, 92, 122, 19, 124, 125...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5RFBIvtbAvf",
        "colab_type": "code",
        "colab": {},
        "outputId": "57a3daab-aa58-4a5a-b20e-11489868b1dd"
      },
      "source": [
        "#check how balanced the dataset is\n",
        "Counter(reviews['rating'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({3: 5077, 4: 13131, 2: 2871, 1: 1565, 0: 842})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jxGCANHbAvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = list(reviews['encoded'])\n",
        "y = list(reviews['rating'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEMEl0mxbAvn",
        "colab_type": "text"
      },
      "source": [
        "#### Pytorch Dataset\n",
        "\n",
        "The dataset is quite straightforward because we’ve already stored our encodings in the input dataframe. We also output the length of the input sequence in each case, because we can have LSTMs that take variable-length sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MiEsz5QbAvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n3Xq81CbAvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = ReviewsDataset(X_train, y_train)\n",
        "valid_ds = ReviewsDataset(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrh3t_sxogqo",
        "colab_type": "text"
      },
      "source": [
        "#### Pytorch training loop\n",
        "The training loop is pretty standard. I’ve used Adam optimizer and cross-entropy loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUqRC-wQbAvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long()\n",
        "            y = y.long()\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
        "\n",
        "def validation_metrics (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    sum_rmse = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long()\n",
        "        y = y.long()\n",
        "        y_hat = model(x, l)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        pred = torch.max(y_hat, 1)[1]\n",
        "        correct += (pred == y).float().sum()\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
        "    return sum_loss/total, correct/total, sum_rmse/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOkgXYMybAv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5000\n",
        "vocab_size = len(words)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zGZpP9low2K",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Model\n",
        "I’ve used 3 variations for the model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oL7C1BdbAv5",
        "colab_type": "text"
      },
      "source": [
        "### 1- LSTM with fixed length input\n",
        "\n",
        "This pretty much has the same structure as the basic LSTM we saw earlier, with the addition of a dropout layer to prevent overfitting. Since we have a classification problem, we have a final linear layer with 5 outputs. This implementation actually works the best among the classification LSTMs, with an accuracy of about 64% and a root-mean-squared-error of only 0.817"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgV19Ze6bAv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_fixed_len(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 5)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e4E2gtybAv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fixed =  LSTM_fixed_len(vocab_size, 50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_kB5b0DbAwA",
        "colab_type": "code",
        "colab": {},
        "outputId": "448e5115-ea63-4d2a-c885-44da0cfdb9c6"
      },
      "source": [
        "train_model(model_fixed, epochs=30, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.239, val loss 1.218, val accuracy 0.556, and val rmse 1.355\n",
            "train loss 1.188, val loss 1.203, val accuracy 0.554, and val rmse 1.348\n",
            "train loss 1.112, val loss 1.165, val accuracy 0.574, and val rmse 1.189\n",
            "train loss 1.072, val loss 1.161, val accuracy 0.517, and val rmse 1.145\n",
            "train loss 1.042, val loss 1.117, val accuracy 0.563, and val rmse 1.279\n",
            "train loss 0.981, val loss 1.113, val accuracy 0.572, and val rmse 1.216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnzKSAFnbAwG",
        "colab_type": "code",
        "colab": {},
        "outputId": "a45d61f6-bd2f-4e79-e8ee-ada060db195e"
      },
      "source": [
        "train_model(model_fixed, epochs=30, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 0.936, val loss 1.065, val accuracy 0.576, and val rmse 1.177\n",
            "train loss 0.846, val loss 1.009, val accuracy 0.603, and val rmse 0.937\n",
            "train loss 0.848, val loss 1.009, val accuracy 0.606, and val rmse 0.912\n",
            "train loss 0.784, val loss 0.994, val accuracy 0.604, and val rmse 0.894\n",
            "train loss 0.741, val loss 0.984, val accuracy 0.617, and val rmse 0.870\n",
            "train loss 0.702, val loss 0.999, val accuracy 0.623, and val rmse 0.863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hANFIkaUbAwK",
        "colab_type": "code",
        "colab": {},
        "outputId": "d903e808-fd1b-4a65-a87e-4ea319adbbc4"
      },
      "source": [
        "train_model(model_fixed, epochs=30, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 0.695, val loss 1.010, val accuracy 0.619, and val rmse 0.869\n",
            "train loss 0.630, val loss 0.992, val accuracy 0.625, and val rmse 0.836\n",
            "train loss 0.583, val loss 1.020, val accuracy 0.632, and val rmse 0.823\n",
            "train loss 0.545, val loss 1.052, val accuracy 0.635, and val rmse 0.823\n",
            "train loss 0.502, val loss 1.088, val accuracy 0.634, and val rmse 0.827\n",
            "train loss 0.469, val loss 1.145, val accuracy 0.639, and val rmse 0.817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPNWqpOdbAwO",
        "colab_type": "text"
      },
      "source": [
        "### 2- LSTM with variable length input\n",
        "\n",
        "We can modify our model a bit to make it accept variable-length inputs. This ends up increasing the training time though, because of the pack_padded_sequence function call which returns a padded batch of variable-length sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IPC4JybAwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_variable_input(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 5)\n",
        "        \n",
        "    def forward(self, x, s):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
        "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
        "        out = self.linear(ht[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO0b0NWybAwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM_variable_input(vocab_size, 50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW1AamJebAwW",
        "colab_type": "code",
        "colab": {},
        "outputId": "994fb263-5763-45fc-f212-036b26ecbb70"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.328, val loss 1.250, val accuracy 0.515, and val rmse 1.312\n",
            "train loss 1.031, val loss 1.063, val accuracy 0.577, and val rmse 1.017\n",
            "train loss 0.904, val loss 0.995, val accuracy 0.603, and val rmse 0.941\n",
            "train loss 0.849, val loss 1.000, val accuracy 0.599, and val rmse 0.940\n",
            "train loss 0.845, val loss 1.009, val accuracy 0.598, and val rmse 0.921\n",
            "train loss 0.834, val loss 1.005, val accuracy 0.593, and val rmse 0.902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zriXqOGSbAwZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "3788e679-9b4c-457e-b05b-cf05488272bc"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 0.828, val loss 1.000, val accuracy 0.599, and val rmse 0.920\n",
            "train loss 0.790, val loss 0.989, val accuracy 0.605, and val rmse 0.894\n",
            "train loss 0.775, val loss 0.992, val accuracy 0.614, and val rmse 0.884\n",
            "train loss 0.755, val loss 0.994, val accuracy 0.597, and val rmse 0.883\n",
            "train loss 0.738, val loss 0.987, val accuracy 0.608, and val rmse 0.872\n",
            "train loss 0.741, val loss 1.005, val accuracy 0.611, and val rmse 0.888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdP-iFiVbAwd",
        "colab_type": "code",
        "colab": {},
        "outputId": "aca34888-77a1-4658-c247-9f48bf79315c"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 0.758, val loss 1.028, val accuracy 0.616, and val rmse 0.884\n",
            "train loss 0.725, val loss 0.994, val accuracy 0.621, and val rmse 0.877\n",
            "train loss 0.715, val loss 0.999, val accuracy 0.607, and val rmse 0.881\n",
            "train loss 0.707, val loss 1.008, val accuracy 0.608, and val rmse 0.879\n",
            "train loss 0.698, val loss 1.018, val accuracy 0.615, and val rmse 0.890\n",
            "train loss 0.686, val loss 1.017, val accuracy 0.603, and val rmse 0.893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAUUf31ibAwg",
        "colab_type": "text"
      },
      "source": [
        "### 3- LSTM with pretrained Glove word embeddings\n",
        "\n",
        "Instead of training our own word embeddings, we can use pre-trained Glove word vectors that have been trained on a massive corpus and probably have better context captured. For our problem, however, this doesn’t seem to help much.\n",
        "\n",
        "Download weights [here](https://nlp.stanford.edu/projects/glove/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6KKZh1MbAwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove_vectors(glove_file=\"./data/glove.6B/glove.6B.50d.txt\"):\n",
        "    \"\"\"Load the glove word vectors\"\"\"\n",
        "    word_vectors = {}\n",
        "    with open(glove_file) as f:\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
        "    return word_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-YXQNq0bAwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
        "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
        "    vocab_size = len(word_counts) + 2\n",
        "    vocab_to_idx = {}\n",
        "    vocab = [\"\", \"UNK\"]\n",
        "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
        "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
        "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
        "    vocab_to_idx[\"UNK\"] = 1\n",
        "    i = 2\n",
        "    for word in word_counts:\n",
        "        if word in word_vecs:\n",
        "            W[i] = word_vecs[word]\n",
        "        else:\n",
        "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
        "        vocab_to_idx[word] = i\n",
        "        vocab.append(word)\n",
        "        i += 1   \n",
        "    return W, np.array(vocab), vocab_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1LHR0aPbAwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vecs = load_glove_vectors()\n",
        "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lDpYxxwbAwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_glove_vecs(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
        "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 5)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNbPjItubAwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MII275yGbAw-",
        "colab_type": "code",
        "colab": {},
        "outputId": "57f8a0eb-1dc5-4bf2-e3f1-d8b3a555f02f"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.281, val loss 1.255, val accuracy 0.556, and val rmse 1.355\n",
            "train loss 1.210, val loss 1.207, val accuracy 0.556, and val rmse 1.354\n",
            "train loss 1.206, val loss 1.204, val accuracy 0.556, and val rmse 1.354\n",
            "train loss 1.201, val loss 1.202, val accuracy 0.556, and val rmse 1.354\n",
            "train loss 1.173, val loss 1.168, val accuracy 0.557, and val rmse 1.352\n",
            "train loss 1.131, val loss 1.122, val accuracy 0.562, and val rmse 1.249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFhC_KhYbAxB",
        "colab_type": "code",
        "colab": {},
        "outputId": "1ca9d6a5-f2ef-4fcd-d3f9-2914c7d326fa"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.112, val loss 1.113, val accuracy 0.556, and val rmse 1.349\n",
            "train loss 1.061, val loss 1.051, val accuracy 0.570, and val rmse 1.109\n",
            "train loss 1.014, val loss 1.014, val accuracy 0.582, and val rmse 1.058\n",
            "train loss 0.979, val loss 0.990, val accuracy 0.599, and val rmse 0.995\n",
            "train loss 0.948, val loss 0.961, val accuracy 0.610, and val rmse 0.950\n",
            "train loss 0.923, val loss 0.952, val accuracy 0.612, and val rmse 0.935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asAZ_c7sbAxH",
        "colab_type": "code",
        "colab": {},
        "outputId": "bff8f680-ca44-4ba5-95a7-b69bb9609111"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.189, val loss 1.014, val accuracy 0.586, and val rmse 1.033\n",
            "train loss 0.946, val loss 0.964, val accuracy 0.606, and val rmse 0.950\n",
            "train loss 0.912, val loss 0.951, val accuracy 0.612, and val rmse 0.941\n",
            "train loss 0.895, val loss 0.949, val accuracy 0.615, and val rmse 0.913\n",
            "train loss 0.886, val loss 0.947, val accuracy 0.617, and val rmse 0.901\n",
            "train loss 0.872, val loss 0.938, val accuracy 0.621, and val rmse 0.890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVK_YzpnbAxL",
        "colab_type": "text"
      },
      "source": [
        "## Predicting ratings using regression instead of classification\n",
        "\n",
        "Since ratings have an order, and a prediction of 3.6 might be better than rounding off to 4 in many cases, it is helpful to explore this as a regression problem. Not surprisingly, this approach gives us the lowest error of just 0.799 because we don’t have just integer predictions anymore.\n",
        "\n",
        "The only change to our model is that instead of the final layer having 5 outputs, we have just one. The training loop changes a bit too, we use MSE loss and we don’t need to take the argmax anymore to get the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6S0JCIBbAxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_regr(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long()\n",
        "            y = y.float()\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss = validation_metrics_regr(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train mse %.3f val rmse %.3f\" % (sum_loss/total, val_loss))\n",
        "\n",
        "def validation_metrics_regr (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long()\n",
        "        y = y.float()\n",
        "        y_hat = model(x, l)\n",
        "        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "    return sum_loss/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8baMAjuCbAxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_regr(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZn861O0bAxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =  LSTM_regr(vocab_size, 50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKGwRHPxbAxX",
        "colab_type": "code",
        "colab": {},
        "outputId": "ff445e78-c864-426b-a0fc-785dc7876dc0"
      },
      "source": [
        "train_model_regr(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train mse 1.663 val rmse 1.313\n",
            "train mse 1.215 val rmse 1.125\n",
            "train mse 1.151 val rmse 1.109\n",
            "train mse 1.114 val rmse 1.115\n",
            "train mse 1.082 val rmse 1.121\n",
            "train mse 1.043 val rmse 1.116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zayWnyUwbAxb",
        "colab_type": "code",
        "colab": {},
        "outputId": "cd046743-fd59-4b06-c7e1-50dee93432f9"
      },
      "source": [
        "train_model_regr(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train mse 1.214 val rmse 1.193\n",
            "train mse 0.884 val rmse 1.032\n",
            "train mse 0.631 val rmse 0.903\n",
            "train mse 0.483 val rmse 0.837\n",
            "train mse 0.416 val rmse 0.806\n",
            "train mse 0.363 val rmse 0.799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD11KcL1pmDv",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion:\n",
        "LSTM appears to be theoretically involved, but its Pytorch implementation is pretty straightforward. Also, while looking at any problem, it is very important to choose the right metric, in our case if we’d gone for accuracy, the model seems to be doing a very bad job, but the RMSE shows that it is off by less than 1 rating point, which is comparable to human performance!\n",
        "\n",
        "References:\n",
        "\n",
        "https://www.usfca.edu/data-institute/certificates/deep-learning-part-one\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "http://web.stanford.edu/class/cs224n/"
      ]
    }
  ]
}